{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉验证\n",
    "\n",
    "验证模型结果的稳定性，采用5重交叉验证，其中每重的训练数据用于训练，测试数据用于测试最后得到平均测试准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_pickled_data(pickled_file):\n",
    "    \"\"\"\n",
    "    load picked data\n",
    "    :param pickled_file:\n",
    "    :return: train_data, train_labels, test_data,\n",
    "             test_labels, valid_data, valid_labels\n",
    "    \"\"\"\n",
    "    with open(pickled_file, 'rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        _train_data = save['train_data']\n",
    "        _train_labels = save['train_labels']\n",
    "        _test_data = save['test_data']\n",
    "        _test_labels = save['test_labels']\n",
    "        _valid_data = save['valid_data']\n",
    "        _valid_labels = save['valid_labels']\n",
    "        del save\n",
    "        print(_train_data.shape, _train_labels.shape)\n",
    "        print(_test_data.shape, _test_labels.shape)\n",
    "        print(_valid_data.shape, _valid_labels.shape)\n",
    "    return _train_data, _train_labels, _test_data, _test_labels, _valid_data, _valid_labels\n",
    "\n",
    "\n",
    "def accuracy_func(predicts, labels):\n",
    "    \"\"\"\n",
    "    total accuracy, digit-wise\n",
    "    :param predicts:\n",
    "    :param labels:\n",
    "    :return: float value, precesion\n",
    "    \"\"\"\n",
    "    _predictions = np.argmax(predicts, 2).T\n",
    "    total_count = 0\n",
    "    for pre, la in zip(_predictions, labels):\n",
    "        for i, j in zip(pre.tolist(), la.tolist()):\n",
    "            if i == j:\n",
    "                total_count += 1\n",
    "    # return 100.0 * np.sum(predictions == labels) / predicts.shape[1] / predicts.shape[0]\n",
    "    return 100.0 * total_count / predicts.shape[1] / predicts.shape[0]\n",
    "\n",
    "\n",
    "def local_contrast_normalization(input_data, image_shape, threshold=1e-4, radius=7):\n",
    "    \"\"\"\n",
    "    Local Contrast Normalization\n",
    "    :param input_data: input data\n",
    "    :param image_shape: image shape\n",
    "    :param threshold: threshold\n",
    "    :param radius: redius\n",
    "    :return: local contrast normalized input data\n",
    "    \"\"\"\n",
    "    # Gaussian filter\n",
    "    filter_shape = radius, radius, image_shape[3], 1\n",
    "    filters = gaussian_initializer(filter_shape)\n",
    "    input_data = tf.convert_to_tensor(input_data, dtype=tf.float32)\n",
    "    convout = tf.nn.conv2d(input_data, filters, [1, 1, 1, 1], 'SAME')\n",
    "    centered_data = tf.sub(input_data, convout)\n",
    "    denoms = tf.sqrt(tf.nn.conv2d(tf.square(centered_data), filters, [1, 1, 1, 1], 'SAME'))\n",
    "    mean = tf.reduce_mean(denoms)\n",
    "    divisor = tf.maximum(mean, denoms)\n",
    "    # Divisise step\n",
    "    new_data = tf.truediv(centered_data, tf.maximum(divisor, threshold))\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def gaussian_initializer(kernel_shape):\n",
    "    \"\"\"\n",
    "    initialize the kernel weights\n",
    "    :param kernel_shape: kernel shape\n",
    "    :return: tensor\n",
    "    \"\"\"\n",
    "    x = np.zeros(kernel_shape, dtype=float)\n",
    "    mid = np.floor(kernel_shape[0] / 2.)\n",
    "    for kernel_idx in range(0, kernel_shape[2]):\n",
    "        for i in range(0, kernel_shape[0]):\n",
    "            for j in range(0, kernel_shape[1]):\n",
    "                x[i, j, kernel_idx, 0] = gaussian(i - mid, j - mid)\n",
    "    return tf.convert_to_tensor(x / np.sum(x), dtype=tf.float32)\n",
    "\n",
    "\n",
    "def gaussian(x, y, sigma=3.0):\n",
    "    \"\"\"\n",
    "    gaussian function\n",
    "    :param x: x value\n",
    "    :param y: y value\n",
    "    :param sigma: sigma\n",
    "    :return: guassian normalized value\n",
    "    \"\"\"\n",
    "    z = 2 * np.pi * sigma ** 2\n",
    "    return 1. / z * np.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiDigits(object):\n",
    "    \"\"\"\n",
    "    Multi Digits Recognition Model\n",
    "    \"\"\"\n",
    "    def __init__(self, picked_file=None, image_size=32, num_labels=11, num_channels=1,\n",
    "                 batch_size=64, patch_size=5, depth_1=16, depth_2=32, depth_3=64,\n",
    "                 hidden_num=64, num_hidden1=64\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param picked_file:\n",
    "        :param image_size:\n",
    "        :param num_labels:\n",
    "        :param num_channels:\n",
    "        :param batch_size:\n",
    "        :param patch_size:\n",
    "        :param depth_1:\n",
    "        :param depth_2:\n",
    "        :param depth_3:\n",
    "        :param hidden_num:\n",
    "        :param num_hidden1:\n",
    "        \"\"\"\n",
    "        self.train_data, self.train_labels, self.test_data, \\\n",
    "            self.test_labels, self.valid_data, self.valid_labels = None, None, None, None, None, None\n",
    "        if picked_file is not None:\n",
    "            self.train_data, self.train_labels, self.test_data, \\\n",
    "                self.test_labels, self.valid_data, self.valid_labels = \\\n",
    "                load_pickled_data(picked_file)\n",
    "        self.train_graph = None\n",
    "        self.infer_graph = tf.Graph()\n",
    "        self.image_size = image_size\n",
    "        self.num_labels = num_labels\n",
    "        self.num_channels = num_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.depth_1 = depth_1\n",
    "        self.depth_2 = depth_2\n",
    "        self.depth_3 = depth_3\n",
    "        self.hidden_num = hidden_num\n",
    "        self.num_hidden1 = num_hidden1\n",
    "        self.shape = [batch_size, image_size, image_size, num_channels]\n",
    "        self.saver = None\n",
    "        self.valid_prediction, self.test_prediction = None, None\n",
    "        self.tf_train_dataset = tf.placeholder(tf.float32, shape=self.shape)\n",
    "        self.tf_train_labels = None\n",
    "        self.tf_valid_dataset = None\n",
    "        self.tf_test_dataset = None\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "        self.train_prediction = None\n",
    "        self.save_path = None\n",
    "        self.infer_saver = None\n",
    "        self.is_inited = False\n",
    "        self.conv_layer1_weights = None\n",
    "        self.conv_layer1_biases = None\n",
    "        self.conv_layer2_weights = None\n",
    "        self.conv_layer2_biases = None\n",
    "        self.conv_layer2_biases = None\n",
    "        self.conv_layer3_weights = None\n",
    "        self.conv_layer3_biases = None\n",
    "        self.out_weights_len = None\n",
    "        self.out_biases_len = None\n",
    "        self.out_weights_1 = None\n",
    "        self.out_biases_1 = None\n",
    "        self.out_weights_2 = None\n",
    "        self.out_weights_2 = None\n",
    "        self.out_biases_2 = None\n",
    "        self.out_weights_3 = None\n",
    "        self.out_biases_3 = None\n",
    "        self.out_weights_4 = None\n",
    "        self.out_biases_4 = None\n",
    "        self.out_weights_5 = None\n",
    "        self.out_biases_5 = None\n",
    "\n",
    "    def init_data(self, picked_file=None):\n",
    "        if picked_file is not None:\n",
    "            self.train_data, self.train_labels, self.test_data, \\\n",
    "                self.test_labels, self.valid_data, self.valid_labels = \\\n",
    "                load_pickled_data(picked_file)\n",
    "\n",
    "    def define_graph(self, keep_pro=0.95, eta=0.05, decay_step=5000, decay_rate=0.95):\n",
    "        \"\"\"\n",
    "        定义图参数\n",
    "        :param keep_pro: DropOut参数\n",
    "        :param eta: 学习率\n",
    "        :param decay_step: 学习率衰减步\n",
    "        :param decay_rate: 学习率衰减率\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.train_graph = tf.Graph()\n",
    "        with self.train_graph.as_default():\n",
    "            # Input Data.\n",
    "            self.tf_train_dataset = tf.placeholder(tf.float32, shape=self.shape)\n",
    "            self.tf_train_labels = tf.placeholder(tf.int32, shape=(self.batch_size, 6))\n",
    "            self.tf_valid_dataset = tf.constant(self.valid_data)\n",
    "            self.tf_test_dataset = tf.constant(self.test_data)\n",
    "            # init varibales\n",
    "            # Conv Layers\n",
    "            self.conv_layer1_weights = tf.get_variable('c_1_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                       self.num_channels, self.depth_1],\n",
    "                                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.conv_layer1_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_1]), name='c_1_b')\n",
    "            self.conv_layer2_weights = tf.get_variable('c_2_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                       self.depth_1, self.depth_2],\n",
    "                                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.conv_layer2_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_2]), name='c_2_b')\n",
    "            self.conv_layer3_weights = tf.get_variable('c_3_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                       self.depth_2, self.num_hidden1],\n",
    "                                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.conv_layer3_biases = tf.Variable(tf.constant(1.0, shape=[self.num_hidden1]), name='c_3_b')\n",
    "            # Output Layer\n",
    "            self.out_weights_len = tf.get_variable('o_len', shape=[self.hidden_num, self.num_labels],\n",
    "                                                   initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_len = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_len'))\n",
    "            self.out_weights_1 = tf.get_variable('o_1', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_1 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_1'))\n",
    "            self.out_weights_2 = tf.get_variable('o_2', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_2 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_2'))\n",
    "            self.out_weights_3 = tf.get_variable('o_3', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_3 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_3'))\n",
    "            self.out_weights_4 = tf.get_variable('o_4', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_4 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_4'))\n",
    "            self.out_weights_5 = tf.get_variable('o_5', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_5 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_5'))\n",
    "            # Training computation.\n",
    "            logitslen, logits1, logits2, logits3, logits4, logits5 = \\\n",
    "                self._infer(self.tf_train_dataset, keep_pro, self.shape)\n",
    "            self.loss = \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logitslen, self.tf_train_labels[:, 0])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, self.tf_train_labels[:, 1])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits2, self.tf_train_labels[:, 2])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits3, self.tf_train_labels[:, 3])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits4, self.tf_train_labels[:, 4])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits5, self.tf_train_labels[:, 5]))\n",
    "            # Optimizer.\n",
    "            global_step = tf.Variable(0)\n",
    "            learning_rate = tf.train.exponential_decay(eta, global_step, decay_step, decay_rate)\n",
    "            self.optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(self.loss, global_step=global_step)\n",
    "            # Predictions of the training, validation, and test data.\n",
    "            self.train_prediction = tf.pack(list(map(tf.nn.softmax,\n",
    "                                                     self._infer(self.tf_train_dataset, 1.0, self.shape))))\n",
    "            self.valid_prediction = tf.pack(list(map(tf.nn.softmax,\n",
    "                                                     self._infer(self.tf_valid_dataset, 1.0, self.shape))))\n",
    "            self.test_prediction = tf.pack(list(map(tf.nn.softmax,\n",
    "                                                    self._infer(self.tf_test_dataset, 1.0, self.shape))))\n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "    def train_model(self, save_path=None, save=True, epoch=50000, verbose=False, cross_validate=False):\n",
    "        \"\"\"\n",
    "        训练模型，部署应用的时候不能调用\n",
    "        :param save_path: ckpt数据保存路径\n",
    "        :param save: 是否保存ckpt数据\n",
    "        :param epoch: 训练迭代次数\n",
    "        :param verbose: 显示迭代过程中的中间结果\n",
    "        :param cross_validate: 是否是交叉验证\n",
    "        :return: epoch_index, losses, mini_batch_acc, valid_batch_acc, test_acc\n",
    "        \"\"\"\n",
    "        epoch_index = []\n",
    "        losses = []\n",
    "        mini_batch_acc = []\n",
    "        valid_batch_acc = []\n",
    "        epochs = epoch\n",
    "        start_time = time.time()\n",
    "        with tf.Session(graph=self.train_graph) as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('Initialized all variables')\n",
    "            for e in range(epochs):\n",
    "                offset = (e * self.batch_size) % (self.train_labels.shape[0] - self.batch_size)\n",
    "                batch_data = self.train_data[offset:(offset + self.batch_size), :, :, :]\n",
    "                batch_labels = self.train_labels[offset:(offset + self.batch_size), :]\n",
    "                feed_dict = {self.tf_train_dataset: batch_data, self.tf_train_labels: batch_labels}\n",
    "                _, l, predictions = sess.run([self.optimizer, self.loss, self.train_prediction], feed_dict=feed_dict)\n",
    "                if e % 1000 == 0:\n",
    "                    epoch_index.append(e)\n",
    "                    mini_acc = accuracy_func(predictions, batch_labels)\n",
    "                    mini_batch_acc.append(mini_acc)\n",
    "                    valid_acc = accuracy_func(self.valid_prediction.eval(), self.valid_labels)\n",
    "                    valid_batch_acc.append(valid_acc)\n",
    "                    losses.append(l)\n",
    "                    if verbose:\n",
    "                        print('Minibatch loss at step %d: %f' % (e, l))\n",
    "                        print('Minibatch accuracy: %.1f%%' % mini_acc)\n",
    "                        if not cross_validate:\n",
    "                            print('Validation accuracy: %.1f%%' % valid_acc)\n",
    "            test_acc = accuracy_func(self.test_prediction.eval(), self.test_labels)\n",
    "            print('Test accuracy: %.1f%%' % test_acc)\n",
    "            if save:\n",
    "                self.save_path = self.saver.save(sess, save_path)\n",
    "                print(\"Model saved in file: %s\" % self.save_path)\n",
    "            end_time = time.time()\n",
    "            print('train time: %s' % (end_time - start_time))\n",
    "        return epoch_index, losses, mini_batch_acc, valid_batch_acc, test_acc\n",
    "\n",
    "    def infer_model(self, input_data, ckpt_path):\n",
    "        \"\"\"\n",
    "        infer input data\n",
    "        :param input_data: input a instance\n",
    "        :param ckpt_path: path to the ckpt file\n",
    "        :return: return result\n",
    "        \"\"\"\n",
    "        infer_graph = tf.Graph()\n",
    "        with infer_graph.as_default():\n",
    "            # Input Data.\n",
    "            tf_infer_data = tf.placeholder(tf.float32, shape=(input_data.shape[0], input_data.shape[1],\n",
    "                                                              input_data.shape[2], 1))\n",
    "            # init varibales\n",
    "            conv_layer1_weights = tf.get_variable('c_1_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.num_channels, self.depth_1],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer1_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_1]), name='c_1_b')\n",
    "            conv_layer2_weights = tf.get_variable('c_2_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.depth_1, self.depth_2],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer2_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_2]), name='c_2_b')\n",
    "            conv_layer3_weights = tf.get_variable('c_3_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.depth_2, self.num_hidden1],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer3_biases = tf.Variable(tf.constant(1.0, shape=[self.num_hidden1]), name='c_3_b')\n",
    "            # Output Layer\n",
    "            out_weights_len = tf.get_variable('o_len', shape=[self.hidden_num, self.num_labels],\n",
    "                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_len = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_len'))\n",
    "            out_weights_1 = tf.get_variable('o_1', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_1 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_1'))\n",
    "            out_weights_2 = tf.get_variable('o_2', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_2 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_2'))\n",
    "            out_weights_3 = tf.get_variable('o_3', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_3 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_3'))\n",
    "            out_weights_4 = tf.get_variable('o_4', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_4 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_4'))\n",
    "            out_weights_5 = tf.get_variable('o_5', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_5 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_5'))\n",
    "\n",
    "            def infer(data, keep_prob, d_shape):\n",
    "                # conv layer\n",
    "                lcn = local_contrast_normalization(data, d_shape)\n",
    "                conv_1 = tf.nn.conv2d(lcn, conv_layer1_weights, [1, 1, 1, 1], 'VALID', name='c_1')\n",
    "                conv_1 = tf.nn.relu(conv_1 + conv_layer1_biases)\n",
    "                conv_1 = tf.nn.local_response_normalization(conv_1)\n",
    "                pool_1 = tf.nn.max_pool(conv_1, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_1')\n",
    "                conv_2 = tf.nn.conv2d(pool_1, conv_layer2_weights, [1, 1, 1, 1], padding='VALID', name='c_2')\n",
    "                conv_2 = tf.nn.relu(conv_2 + conv_layer2_biases)\n",
    "                conv_2 = tf.nn.local_response_normalization(conv_2)\n",
    "                pool_2 = tf.nn.max_pool(conv_2, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_2_')\n",
    "                conv_3 = tf.nn.conv2d(pool_2, conv_layer3_weights, [1, 1, 1, 1], padding='VALID', name='c_3')\n",
    "                conv_3 = tf.nn.relu(conv_3 + conv_layer3_biases)\n",
    "                conv_3 = tf.nn.dropout(conv_3, keep_prob)\n",
    "                shapes = conv_3.get_shape().as_list()\n",
    "                hidden = tf.reshape(conv_3, [shapes[0], shapes[1] * shapes[2] * shapes[3]])\n",
    "                # fc layer\n",
    "                logits_len = tf.matmul(hidden, out_weights_len) + out_biases_len\n",
    "                logits_1 = tf.matmul(hidden, out_weights_1) + out_biases_1\n",
    "                logits_2 = tf.matmul(hidden, out_weights_2) + out_biases_2\n",
    "                logits_3 = tf.matmul(hidden, out_weights_3) + out_biases_3\n",
    "                logits_4 = tf.matmul(hidden, out_weights_4) + out_biases_4\n",
    "                logits_5 = tf.matmul(hidden, out_weights_5) + out_biases_5\n",
    "                return logits_len, logits_1, logits_2, logits_3, logits_4, logits_5\n",
    "\n",
    "            logitslen, logits1, logits2, logits3, logits4, logits5 = infer(tf_infer_data, 1.0, self.shape)\n",
    "            # Predictions\n",
    "            softmax_result = list(map(tf.nn.softmax, infer(tf_infer_data, 1.0, self.shape)))\n",
    "            infer_predict = tf.pack(softmax_result)\n",
    "            prediction = tf.transpose(tf.argmax(infer_predict, 2))\n",
    "            self.infer_saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session(graph=infer_graph) as session:\n",
    "            self.infer_saver.restore(session, save_path=ckpt_path)\n",
    "            input_prediction, infer_prediction, l_len, l_1, l_2, l_3, l_4, l_5 = session.run(\n",
    "                [prediction, infer_predict, logitslen, logits1, logits2, logits3, logits4, logits5],\n",
    "                feed_dict={tf_infer_data: input_data})\n",
    "            logits_output = np.array([l_len, l_1, l_2, l_3, l_4, l_5]).reshape((6, 11))\n",
    "            softmax = lambda data: np.exp(data) / np.sum(np.exp(data))\n",
    "            df = pd.DataFrame(np.array([softmax(d) for d in logits_output]).T,\n",
    "                              columns=['length', '1', '2', '3', '4', '5'])\n",
    "            value = list(map(str, range(10)))\n",
    "            value.append('no digit')\n",
    "            df.insert(0, column='softmax', value=value)\n",
    "            return input_prediction, df\n",
    "\n",
    "    def infer_data(self, input_data, input_labels, ckpt_path):\n",
    "        \"\"\"\n",
    "        infer input data\n",
    "        :param input_data: input a instance\n",
    "        :param ckpt_path: path to the ckpt file\n",
    "        :param input_labels: input data labels\n",
    "        :return: return result\n",
    "        \"\"\"\n",
    "        infer_graph = tf.Graph()\n",
    "        with infer_graph.as_default():\n",
    "            # Input Data.\n",
    "            tf_infer_data = tf.placeholder(tf.float32, shape=(input_data.shape[0], input_data.shape[1],\n",
    "                                                              input_data.shape[2], 1))\n",
    "            tf_infer_label = tf.placeholder(tf.int32, shape=(input_labels.shape[0], input_labels.shape[1]))\n",
    "            # init varibales\n",
    "            conv_layer1_weights = tf.get_variable('c_1_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.num_channels, self.depth_1],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer1_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_1]), name='c_1_b')\n",
    "            conv_layer2_weights = tf.get_variable('c_2_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.depth_1, self.depth_2],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer2_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_2]), name='c_2_b')\n",
    "            conv_layer3_weights = tf.get_variable('c_3_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.depth_2, self.num_hidden1],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer3_biases = tf.Variable(tf.constant(1.0, shape=[self.num_hidden1]), name='c_3_b')\n",
    "            # Output Layer\n",
    "            out_weights_len = tf.get_variable('o_len', shape=[self.hidden_num, self.num_labels],\n",
    "                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_len = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_len'))\n",
    "            out_weights_1 = tf.get_variable('o_1', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_1 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_1'))\n",
    "            out_weights_2 = tf.get_variable('o_2', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_2 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_2'))\n",
    "            out_weights_3 = tf.get_variable('o_3', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_3 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_3'))\n",
    "            out_weights_4 = tf.get_variable('o_4', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_4 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_4'))\n",
    "            out_weights_5 = tf.get_variable('o_5', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_5 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_5'))\n",
    "\n",
    "            def infer(data, keep_prob, d_shape):\n",
    "                # conv layer\n",
    "                lcn = local_contrast_normalization(data, d_shape)\n",
    "                conv_1 = tf.nn.conv2d(lcn, conv_layer1_weights, [1, 1, 1, 1], 'VALID', name='c_1')\n",
    "                conv_1 = tf.nn.relu(conv_1 + conv_layer1_biases)\n",
    "                conv_1 = tf.nn.local_response_normalization(conv_1)\n",
    "                pool_1 = tf.nn.max_pool(conv_1, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_1')\n",
    "                conv_2 = tf.nn.conv2d(pool_1, conv_layer2_weights, [1, 1, 1, 1], padding='VALID', name='c_2')\n",
    "                conv_2 = tf.nn.relu(conv_2 + conv_layer2_biases)\n",
    "                conv_2 = tf.nn.local_response_normalization(conv_2)\n",
    "                pool_2 = tf.nn.max_pool(conv_2, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_2_')\n",
    "                conv_3 = tf.nn.conv2d(pool_2, conv_layer3_weights, [1, 1, 1, 1], padding='VALID', name='c_3')\n",
    "                conv_3 = tf.nn.relu(conv_3 + conv_layer3_biases)\n",
    "                conv_3 = tf.nn.dropout(conv_3, keep_prob)\n",
    "                shapes = conv_3.get_shape().as_list()\n",
    "                hidden = tf.reshape(conv_3, [shapes[0], shapes[1] * shapes[2] * shapes[3]])\n",
    "                # fc layer\n",
    "                logits_len = tf.matmul(hidden, out_weights_len) + out_biases_len\n",
    "                logits_1 = tf.matmul(hidden, out_weights_1) + out_biases_1\n",
    "                logits_2 = tf.matmul(hidden, out_weights_2) + out_biases_2\n",
    "                logits_3 = tf.matmul(hidden, out_weights_3) + out_biases_3\n",
    "                logits_4 = tf.matmul(hidden, out_weights_4) + out_biases_4\n",
    "                logits_5 = tf.matmul(hidden, out_weights_5) + out_biases_5\n",
    "                return logits_len, logits_1, logits_2, logits_3, logits_4, logits_5\n",
    "\n",
    "            logitslen, logits1, logits2, logits3, logits4, logits5 = infer(tf_infer_data, 1.0, self.shape)\n",
    "\n",
    "            # Predictions\n",
    "            _loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logitslen, tf_infer_label[:, 0])) + \\\n",
    "                    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, tf_infer_label[:, 1])) + \\\n",
    "                    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits2, tf_infer_label[:, 2])) + \\\n",
    "                    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits3, tf_infer_label[:, 3])) + \\\n",
    "                    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits4, tf_infer_label[:, 4])) + \\\n",
    "                    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits5, tf_infer_label[:, 5]))\n",
    "            infer_predict = tf.pack(list(map(tf.nn.softmax, infer(tf_infer_data, 1.0, self.shape))))\n",
    "            prediction = tf.transpose(tf.argmax(infer_predict, 2))\n",
    "            self.infer_saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session(graph=infer_graph) as session:\n",
    "            self.infer_saver.restore(session, save_path=ckpt_path)\n",
    "            input_prediction, infer_prediction, loss = session.run([prediction, infer_predict, _loss],\n",
    "                                                                   feed_dict={tf_infer_data: input_data,\n",
    "                                                                              tf_infer_label: input_labels})\n",
    "            accuracy = accuracy_func(infer_prediction, input_labels[:])\n",
    "            return input_prediction, loss, accuracy\n",
    "\n",
    "    def _infer(self, data, keep_prob, d_shape):\n",
    "        \"\"\"\n",
    "        same as infer_data, for training process\n",
    "        :param data: data\n",
    "        :param keep_prob: keep probability for DropOut\n",
    "        :param d_shape: shape of data\n",
    "        :return: logits_1, logits_2, logits_3, logits_4, logits_5\n",
    "        \"\"\"\n",
    "        # conv layer\n",
    "        lcn = local_contrast_normalization(data, d_shape)\n",
    "        conv_1 = tf.nn.conv2d(lcn, self.conv_layer1_weights, [1, 1, 1, 1], 'VALID', name='c_1')\n",
    "        conv_1 = tf.nn.relu(conv_1 + self.conv_layer1_biases)\n",
    "        lrn = tf.nn.local_response_normalization(conv_1)\n",
    "        pool_1 = tf.nn.max_pool(lrn, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_1')\n",
    "        conv_2 = tf.nn.conv2d(pool_1, self.conv_layer2_weights, [1, 1, 1, 1], padding='VALID', name='c_2')\n",
    "        conv_2 = tf.nn.relu(conv_2 + self.conv_layer2_biases)\n",
    "        lrn = tf.nn.local_response_normalization(conv_2)\n",
    "        pool_2 = tf.nn.max_pool(lrn, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_2_')\n",
    "        conv_3 = tf.nn.conv2d(pool_2, self.conv_layer3_weights, [1, 1, 1, 1], padding='VALID', name='c_3')\n",
    "        conv_3 = tf.nn.relu(conv_3 + self.conv_layer3_biases)\n",
    "        conv_3 = tf.nn.dropout(conv_3, keep_prob)\n",
    "        shapes = conv_3.get_shape().as_list()\n",
    "        hidden = tf.reshape(conv_3, [shapes[0], shapes[1] * shapes[2] * shapes[3]])\n",
    "        # fc layer\n",
    "        logits_len = tf.matmul(hidden, self.out_weights_len) + self.out_biases_len\n",
    "        logits_1 = tf.matmul(hidden, self.out_weights_1) + self.out_biases_1\n",
    "        logits_2 = tf.matmul(hidden, self.out_weights_2) + self.out_biases_2\n",
    "        logits_3 = tf.matmul(hidden, self.out_weights_3) + self.out_biases_3\n",
    "        logits_4 = tf.matmul(hidden, self.out_weights_4) + self.out_biases_4\n",
    "        logits_5 = tf.matmul(hidden, self.out_weights_5) + self.out_biases_5\n",
    "        return logits_len, logits_1, logits_2, logits_3, logits_4, logits_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((230070, 32, 32, 1), (230070, 6))\n",
      "((13068, 32, 32, 1), (13068, 6))\n",
      "((5684, 32, 32, 1), (5684, 6))\n",
      "('all data shape', (248822, 32, 32, 1))\n",
      "('all lables shape', (248822, 6))\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels, test_data, test_labels, valid_data, valid_labels = \\\n",
    "        load_pickled_data('SVHN.pickle')\n",
    "\n",
    "def delete_data(train_model):\n",
    "    del train_model.train_data\n",
    "    del train_model.train_labels\n",
    "    del train_model.test_data\n",
    "    del train_model.test_labels\n",
    "    del train_model.valid_data\n",
    "    del train_model.valid_labels\n",
    "\n",
    "all_data = np.concatenate((train_data, test_data, valid_data))\n",
    "all_labels = np.concatenate((train_labels, test_labels, valid_labels))\n",
    "\n",
    "del train_data, train_labels, test_data, test_labels, valid_data, valid_labels\n",
    "print(\"all data shape\", all_data.shape)\n",
    "print(\"all lables shape\", all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  7  8 10 10 10]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGhoGRCPhhoaGhgHRSLihoaFhQPw/f+qbcwL7ri8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x765ac50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(all_data[0].reshape(32, 32), cmap=plt.cm.Greys)\n",
    "print(all_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init(random_state):\n",
    "    train_model = MultiDigits()\n",
    "    # 先赋值，再定义图\n",
    "    train_model.train_data, train_model.test_data, train_model.train_labels, train_model.test_labels = \\\n",
    "            train_test_split(all_data, all_labels, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    # 由于GPU的Memory没有那么大，因此这里取部分数据\n",
    "    train_model.valid_data, train_model.valid_labels = \\\n",
    "        train_model.test_data[9000:18000,...], train_model.test_labels[9000:18000,...]\n",
    "    \n",
    "    train_model.test_data, train_model.test_labels = \\\n",
    "        train_model.test_data[0:9000,...], train_model.test_labels[0:9000,...]\n",
    "    # 定义图\n",
    "    train_model.define_graph(keep_pro=0.95, eta=0.05, decay_step=5000, decay_rate=0.95)\n",
    "    print(train_model.train_data.shape, train_model.train_labels.shape)\n",
    "    print(train_model.test_data.shape, train_model.test_labels.shape)\n",
    "    print(train_model.valid_data.shape, train_model.valid_labels.shape)\n",
    "    \n",
    "    return train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized all variables\n",
      "Minibatch loss at step 0: 20.166700\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 52.2%\n",
      "Minibatch loss at step 1000: 5.066279\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 2000: 2.601651\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 3000: 2.751622\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 4000: 4.001742\n",
      "Minibatch accuracy: 81.0%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 5000: 2.498382\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 6000: 2.621090\n",
      "Minibatch accuracy: 89.3%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 7000: 2.119305\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8000: 2.210606\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 9000: 1.886064\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 10000: 1.836048\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 11000: 1.572184\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 12000: 1.979546\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 13000: 1.507737\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 14000: 1.718782\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 15000: 2.519317\n",
      "Minibatch accuracy: 89.6%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 16000: 1.764904\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 17000: 1.632653\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 18000: 1.735879\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 19000: 1.615450\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 20000: 1.678793\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 21000: 1.954816\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 22000: 1.386874\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 23000: 1.954365\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 24000: 1.298775\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 25000: 1.283319\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 26000: 1.260939\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 27000: 1.285141\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 28000: 1.586641\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 29000: 1.430638\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 30000: 1.568856\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 31000: 1.966915\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 32000: 1.577002\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 33000: 2.227981\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 34000: 1.435918\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 35000: 1.544330\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 36000: 1.248218\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 37000: 1.640359\n",
      "Minibatch accuracy: 92.7%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 38000: 1.180539\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 39000: 1.502620\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 40000: 1.305551\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 41000: 1.432048\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 42000: 1.529109\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 43000: 1.731292\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 44000: 1.249054\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 45000: 1.616468\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 46000: 1.576203\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 47000: 1.659695\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 48000: 1.308700\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 49000: 1.600871\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 92.9%\n",
      "Test accuracy: 93.2%\n",
      "Model saved in file: ckpt_data/SVHN.ckpt\n",
      "train time: 673.527858973\n"
     ]
    }
   ],
   "source": [
    "# train it\n",
    "epoch_index_test, losses_test, mini_batch_acc_test, valid_batch_acc_test, test_acc_test = \\\n",
    "        train_model.train_model(save_path='ckpt_data/SVHN.ckpt', save=True, epoch=50000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> cross validation: 0\n",
      "((199057, 32, 32, 1), (199057, 6))\n",
      "((9000, 32, 32, 1), (9000, 6))\n",
      "((9000, 32, 32, 1), (9000, 6))\n",
      "Initialized all variables\n",
      "Minibatch loss at step 0: 16.258226\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 52.9%\n",
      "Minibatch loss at step 1000: 4.001462\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 2000: 2.409515\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 3000: 1.909559\n",
      "Minibatch accuracy: 92.7%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 4000: 2.117732\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 5000: 1.557809\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 6000: 1.665587\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 7000: 2.144474\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 8000: 1.858371\n",
      "Minibatch accuracy: 92.7%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 9000: 1.661477\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 10000: 1.306011\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 11000: 1.978533\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 12000: 1.370130\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 13000: 1.695269\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 14000: 1.084336\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 15000: 1.457256\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 16000: 1.309169\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 17000: 1.523667\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 18000: 1.764772\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 19000: 1.801788\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 20000: 1.058348\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 21000: 1.174930\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 22000: 1.517597\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 23000: 1.601682\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 24000: 1.249026\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 25000: 1.336336\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 26000: 0.871835\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 27000: 1.609454\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 28000: 1.478752\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 29000: 1.047678\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 30000: 2.143445\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 31000: 1.621852\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 32000: 1.323336\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 33000: 0.866643\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 34000: 1.518006\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 35000: 1.293246\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 36000: 1.076197\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 37000: 1.238621\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 38000: 0.863539\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 39000: 0.885529\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 40000: 1.271148\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 41000: 1.132346\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 42000: 1.687914\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 43000: 1.235881\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 44000: 0.835319\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 94.1%\n",
      "Minibatch loss at step 45000: 1.151199\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 46000: 1.272397\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 94.2%\n",
      "Minibatch loss at step 47000: 1.162613\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 48000: 2.108589\n",
      "Minibatch accuracy: 89.3%\n",
      "Validation accuracy: 94.1%\n",
      "Minibatch loss at step 49000: 1.240417\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 94.2%\n",
      "Test accuracy: 94.1%\n",
      "Model saved in file: cross_0/SVHN.ckpt\n",
      "train time: 671.002494097\n",
      ">>> cross validation: 1\n",
      "((199057, 32, 32, 1), (199057, 6))\n",
      "((9000, 32, 32, 1), (9000, 6))\n",
      "((9000, 32, 32, 1), (9000, 6))\n",
      "Initialized all variables\n",
      "Minibatch loss at step 0: 18.685349\n",
      "Minibatch accuracy: 28.6%\n",
      "Validation accuracy: 51.4%\n",
      "Minibatch loss at step 1000: 3.938753\n",
      "Minibatch accuracy: 80.7%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 2000: 2.744492\n",
      "Minibatch accuracy: 85.7%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 3000: 2.016618\n",
      "Minibatch accuracy: 89.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 4000: 1.850858\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 5000: 1.630504\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 6000: 1.667690\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 7000: 2.092466\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 8000: 1.394175\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 9000: 1.395346\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 10000: 1.304662\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 11000: 1.803186\n",
      "Minibatch accuracy: 92.7%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 12000: 1.475988\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 13000: 0.995361\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 14000: 1.276208\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 15000: 1.398532\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 16000: 1.366952\n",
      "Minibatch accuracy: 92.7%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 17000: 1.263059\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 18000: 0.970574\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 19000: 1.383848\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 20000: 1.613370\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 21000: 1.640834\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 22000: 1.811099\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 23000: 1.431849\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 24000: 1.185823\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 25000: 1.363629\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 26000: 1.013017\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 27000: 1.406310\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 28000: 1.000115\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 29000: 1.382616\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 30000: 1.763875\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 31000: 1.274119\n",
      "Minibatch accuracy: 92.7%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 32000: 0.993151\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 33000: 1.502209\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 34000: 1.389861\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 35000: 1.604592\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 94.2%\n",
      "Minibatch loss at step 36000: 0.619565\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 94.1%\n",
      "Minibatch loss at step 37000: 1.537813\n",
      "Minibatch accuracy: 92.7%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 38000: 1.624895\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 94.3%\n",
      "Minibatch loss at step 39000: 1.029396\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 94.3%\n",
      "Minibatch loss at step 40000: 1.128761\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 94.3%\n",
      "Minibatch loss at step 41000: 1.145785\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 94.3%\n",
      "Minibatch loss at step 42000: 0.921982\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 43000: 1.142128\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 44000: 1.056072\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 45000: 1.184137\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 46000: 1.493176\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 47000: 1.189887\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 48000: 1.776295\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 49000: 0.862792\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 94.5%\n",
      "Test accuracy: 94.4%\n",
      "Model saved in file: cross_1/SVHN.ckpt\n",
      "train time: 670.887355804\n",
      ">>> cross validation: 2\n",
      "((199057, 32, 32, 1), (199057, 6))\n",
      "((9000, 32, 32, 1), (9000, 6))\n",
      "((9000, 32, 32, 1), (9000, 6))\n",
      "Initialized all variables\n",
      "Minibatch loss at step 0: 17.244612\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 53.1%\n",
      "Minibatch loss at step 1000: 4.197104\n",
      "Minibatch accuracy: 77.1%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 2000: 3.468979\n",
      "Minibatch accuracy: 82.3%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 3000: 2.867542\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 4000: 1.808665\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 5000: 2.427790\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 6000: 1.811635\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 7000: 2.440065\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 8000: 2.044470\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 9000: 1.636169\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 10000: 1.779835\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 11000: 2.469600\n",
      "Minibatch accuracy: 88.5%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 12000: 1.324332\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 13000: 1.630718\n",
      "Minibatch accuracy: 92.7%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 14000: 1.849904\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 15000: 1.764933\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 16000: 1.657850\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 17000: 1.472960\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 18000: 1.179181\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 19000: 1.591211\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 20000: 1.461523\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 21000: 1.640993\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 22000: 1.376736\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 23000: 1.630982\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 24000: 1.154083\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 25000: 1.248380\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 26000: 1.164464\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 27000: 1.407883\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 28000: 1.116696\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 29000: 0.954506\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 30000: 1.293927\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 31000: 1.329323\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 32000: 0.935082\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 33000: 1.902374\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 34000: 1.251698\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 35000: 1.234673\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 36000: 0.795683\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 94.1%\n",
      "Minibatch loss at step 37000: 1.283673\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 38000: 0.933111\n",
      "Minibatch accuracy: 97.4%\n",
      "Validation accuracy: 94.1%\n",
      "Minibatch loss at step 39000: 1.067433\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 94.2%\n",
      "Minibatch loss at step 40000: 1.174743\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 94.2%\n",
      "Minibatch loss at step 41000: 1.079526\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 94.2%\n",
      "Minibatch loss at step 42000: 1.813893\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 94.3%\n",
      "Minibatch loss at step 43000: 1.365916\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 94.3%\n",
      "Minibatch loss at step 44000: 1.282132\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 94.3%\n",
      "Minibatch loss at step 45000: 0.835714\n",
      "Minibatch accuracy: 97.9%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 46000: 1.254946\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 47000: 1.542583\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 94.3%\n",
      "Minibatch loss at step 48000: 1.194117\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 49000: 1.185170\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 94.4%\n",
      "Test accuracy: 94.4%\n",
      "Model saved in file: cross_2/SVHN.ckpt\n",
      "train time: 670.880625963\n",
      ">>> cross validation: 3\n",
      "((199057, 32, 32, 1), (199057, 6))\n",
      "((9000, 32, 32, 1), (9000, 6))\n",
      "((9000, 32, 32, 1), (9000, 6))\n",
      "Initialized all variables\n",
      "Minibatch loss at step 0: 16.208744\n",
      "Minibatch accuracy: 26.3%\n",
      "Validation accuracy: 54.0%\n",
      "Minibatch loss at step 1000: 4.549571\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 2000: 3.319654\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 3000: 2.600915\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 4000: 2.634127\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 5000: 2.106213\n",
      "Minibatch accuracy: 88.5%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 6000: 1.110108\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 7000: 1.820818\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 8000: 1.576186\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 9000: 1.966775\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 10000: 1.143759\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 11000: 1.656317\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 12000: 1.562723\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 13000: 1.441193\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 14000: 1.381412\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 15000: 1.485570\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 16000: 1.374895\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 17000: 1.191786\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 18000: 1.860960\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 19000: 0.943422\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 20000: 1.506724\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 21000: 1.100242\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 22000: 1.429121\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 23000: 1.274792\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 24000: 1.344429\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 25000: 1.294057\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 26000: 1.388361\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 27000: 1.517521\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 28000: 1.437795\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 29000: 1.833102\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 30000: 1.082306\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 31000: 1.294750\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 32000: 1.102473\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 33000: 1.349809\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 34000: 1.387421\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 35000: 1.175402\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 36000: 1.757595\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 37000: 1.194297\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 38000: 1.263090\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 39000: 1.096075\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 40000: 1.061633\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 41000: 1.401495\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 42000: 0.875046\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 43000: 1.059786\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 44000: 1.025635\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 45000: 0.902913\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 94.1%\n",
      "Minibatch loss at step 46000: 1.270893\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 47000: 1.416326\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 94.1%\n",
      "Minibatch loss at step 48000: 0.913456\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 94.1%\n",
      "Minibatch loss at step 49000: 1.038429\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 94.1%\n",
      "Test accuracy: 94.4%\n",
      "Model saved in file: cross_3/SVHN.ckpt\n",
      "train time: 671.540694952\n",
      ">>> cross validation: 4\n",
      "((199057, 32, 32, 1), (199057, 6))\n",
      "((9000, 32, 32, 1), (9000, 6))\n",
      "((9000, 32, 32, 1), (9000, 6))\n",
      "Initialized all variables\n",
      "Minibatch loss at step 0: 20.803030\n",
      "Minibatch accuracy: 3.6%\n",
      "Validation accuracy: 44.4%\n",
      "Minibatch loss at step 1000: 4.681136\n",
      "Minibatch accuracy: 75.5%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 2000: 2.737580\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3000: 3.043648\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 4000: 2.019385\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 5000: 2.603971\n",
      "Minibatch accuracy: 88.5%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 6000: 1.804485\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 7000: 2.107604\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 8000: 1.937443\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 9000: 1.721345\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 10000: 1.199523\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 11000: 1.845720\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 12000: 1.470666\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 13000: 1.641961\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 14000: 1.350673\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 15000: 1.130035\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 16000: 2.247674\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 17000: 1.488457\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 18000: 1.638234\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 19000: 1.290275\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 20000: 1.963251\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 21000: 1.560474\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 22000: 1.157111\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 23000: 2.179419\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 24000: 1.339182\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 25000: 2.083908\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 26000: 1.036486\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 27000: 1.017400\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 28000: 1.658531\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 29000: 1.319452\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 30000: 1.306007\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 31000: 1.813735\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 32000: 1.510355\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 33000: 0.930049\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 34000: 1.111376\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 35000: 1.543393\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 36000: 1.771473\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 37000: 1.548029\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 38000: 1.322533\n",
      "Minibatch accuracy: 92.7%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 39000: 0.996035\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 40000: 1.148948\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 41000: 1.269059\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 42000: 1.034150\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 43000: 1.248821\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 44000: 1.683004\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 45000: 1.888999\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 46000: 0.875211\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 47000: 0.962320\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 48000: 1.416575\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 49000: 1.347568\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.8%\n",
      "Test accuracy: 93.7%\n",
      "Model saved in file: cross_4/SVHN.ckpt\n",
      "train time: 670.962749004\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross validataion\n",
    "cv_result = {}\n",
    "\n",
    "for i in range(5):\n",
    "    print('>>> cross validation: %s' % i)\n",
    "    train_model = init(i)\n",
    "    dir_name = 'cross_%s' % i\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    epoch_index, losses, mini_batch_acc, valid_batch_acc, test_acc = \\\n",
    "        train_model.train_model(save_path='cross_%s/SVHN.ckpt' % i, save=True, epoch=50000, verbose=True)\n",
    "    cv_result[i] = {'epoch_index': epoch_index, 'losses': losses, 'mini_batch_acc': mini_batch_acc, \\\n",
    "                    'valid_batch_acc': valid_batch_acc, 'test_acc': test_acc}\n",
    "    del train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 94.07592592592592)\n",
      "(1, 94.38703703703703)\n",
      "(2, 94.36296296296297)\n",
      "(3, 94.4037037037037)\n",
      "(4, 93.65185185185186)\n"
     ]
    }
   ],
   "source": [
    "for k, v in cv_result.iteritems():\n",
    "    print(k, v['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average test accuracy: 94.1762962963\n"
     ]
    }
   ],
   "source": [
    "print('average test accuracy: %s' % (sum([v['test_acc'] for _, v in cv_result.iteritems()]) / len(cv_result.keys())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
