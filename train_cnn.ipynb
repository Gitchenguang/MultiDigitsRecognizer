{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_pickled_data(pickled_file):\n",
    "    \"\"\"\n",
    "    load picked data\n",
    "    :param pickled_file:\n",
    "    :return: train_data, train_labels, test_data,\n",
    "             test_labels, valid_data, valid_labels\n",
    "    \"\"\"\n",
    "    with open(pickled_file, 'rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        _train_data = save['train_data']\n",
    "        _train_labels = save['train_labels']\n",
    "        _test_data = save['test_data']\n",
    "        _test_labels = save['test_labels']\n",
    "        _valid_data = save['valid_data']\n",
    "        _valid_labels = save['valid_labels']\n",
    "        del save\n",
    "        print(_train_data.shape, _train_labels.shape)\n",
    "        print(_test_data.shape, _test_labels.shape)\n",
    "        print(_valid_data.shape, _valid_labels.shape)\n",
    "    return _train_data, _train_labels, _test_data, _test_labels, _valid_data, _valid_labels\n",
    "\n",
    "\n",
    "def accuracy_func(predicts, labels):\n",
    "    \"\"\"\n",
    "    total accuracy, digit-wise\n",
    "    :param predicts:\n",
    "    :param labels:\n",
    "    :return: float value, precesion\n",
    "    \"\"\"\n",
    "    _predictions = np.argmax(predicts, 2).T\n",
    "    total_count = 0\n",
    "    for pre, la in zip(_predictions, labels):\n",
    "        for i, j in zip(pre.tolist(), la.tolist()):\n",
    "            if i == j:\n",
    "                total_count += 1\n",
    "    # return 100.0 * np.sum(predictions == labels) / predicts.shape[1] / predicts.shape[0]\n",
    "    return 100.0 * total_count / predicts.shape[1] / predicts.shape[0]\n",
    "\n",
    "\n",
    "def local_contrast_normalization(input_data, image_shape, threshold=1e-4, radius=7):\n",
    "    \"\"\"\n",
    "    Local Contrast Normalization\n",
    "    :param input_data: input data\n",
    "    :param image_shape: image shape\n",
    "    :param threshold: threshold\n",
    "    :param radius: redius\n",
    "    :return: local contrast normalized input data\n",
    "    \"\"\"\n",
    "    # Gaussian filter\n",
    "    filter_shape = radius, radius, image_shape[3], 1\n",
    "    filters = gaussian_initializer(filter_shape)\n",
    "    input_data = tf.convert_to_tensor(input_data, dtype=tf.float32)\n",
    "    convout = tf.nn.conv2d(input_data, filters, [1, 1, 1, 1], 'SAME')\n",
    "    centered_data = tf.sub(input_data, convout)\n",
    "    denoms = tf.sqrt(tf.nn.conv2d(tf.square(centered_data), filters, [1, 1, 1, 1], 'SAME'))\n",
    "    mean = tf.reduce_mean(denoms)\n",
    "    divisor = tf.maximum(mean, denoms)\n",
    "    # Divisise step\n",
    "    new_data = tf.truediv(centered_data, tf.maximum(divisor, threshold))\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def gaussian_initializer(kernel_shape):\n",
    "    \"\"\"\n",
    "    initialize the kernel weights\n",
    "    :param kernel_shape: kernel shape\n",
    "    :return: tensor\n",
    "    \"\"\"\n",
    "    x = np.zeros(kernel_shape, dtype=float)\n",
    "    mid = np.floor(kernel_shape[0] / 2.)\n",
    "    for kernel_idx in range(0, kernel_shape[2]):\n",
    "        for i in range(0, kernel_shape[0]):\n",
    "            for j in range(0, kernel_shape[1]):\n",
    "                x[i, j, kernel_idx, 0] = gaussian(i - mid, j - mid)\n",
    "    return tf.convert_to_tensor(x / np.sum(x), dtype=tf.float32)\n",
    "\n",
    "\n",
    "def gaussian(x, y, sigma=3.0):\n",
    "    \"\"\"\n",
    "    gaussian function\n",
    "    :param x: x value\n",
    "    :param y: y value\n",
    "    :param sigma: sigma\n",
    "    :return: guassian normalized value\n",
    "    \"\"\"\n",
    "    z = 2 * np.pi * sigma ** 2\n",
    "    return 1. / z * np.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiDigits(object):\n",
    "    \"\"\"\n",
    "    Multi Digits Recognition Model\n",
    "    \"\"\"\n",
    "    def __init__(self, picked_file=None, image_size=32, num_labels=11, num_channels=1,\n",
    "                 batch_size=64, patch_size=5, depth_1=16, depth_2=32, depth_3=64,\n",
    "                 hidden_num=64, num_hidden1=64\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param picked_file:\n",
    "        :param image_size:\n",
    "        :param num_labels:\n",
    "        :param num_channels:\n",
    "        :param batch_size:\n",
    "        :param patch_size:\n",
    "        :param depth_1:\n",
    "        :param depth_2:\n",
    "        :param depth_3:\n",
    "        :param hidden_num:\n",
    "        :param num_hidden1:\n",
    "        \"\"\"\n",
    "        self.train_data, self.train_labels, self.test_data, \\\n",
    "            self.test_labels, self.valid_data, self.valid_labels = None, None, None, None, None, None\n",
    "        if picked_file is not None:\n",
    "            self.train_data, self.train_labels, self.test_data, \\\n",
    "                self.test_labels, self.valid_data, self.valid_labels = \\\n",
    "                load_pickled_data(picked_file)\n",
    "        self.train_graph = None\n",
    "        self.infer_graph = tf.Graph()\n",
    "        self.image_size = image_size\n",
    "        self.num_labels = num_labels\n",
    "        self.num_channels = num_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.depth_1 = depth_1\n",
    "        self.depth_2 = depth_2\n",
    "        self.depth_3 = depth_3\n",
    "        self.hidden_num = hidden_num\n",
    "        self.num_hidden1 = num_hidden1\n",
    "        self.shape = [batch_size, image_size, image_size, num_channels]\n",
    "        self.saver = None\n",
    "        self.valid_prediction, self.test_prediction = None, None\n",
    "        self.tf_train_dataset = tf.placeholder(tf.float32, shape=self.shape)\n",
    "        self.tf_train_labels = None\n",
    "        self.tf_valid_dataset = None\n",
    "        self.tf_test_dataset = None\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "        self.train_prediction = None\n",
    "        self.save_path = None\n",
    "        self.infer_saver = None\n",
    "        self.is_inited = False\n",
    "        self.conv_layer1_weights = None\n",
    "        self.conv_layer1_biases = None\n",
    "        self.conv_layer2_weights = None\n",
    "        self.conv_layer2_biases = None\n",
    "        self.conv_layer2_biases = None\n",
    "        self.conv_layer3_weights = None\n",
    "        self.conv_layer3_biases = None\n",
    "        self.out_weights_len = None\n",
    "        self.out_biases_len = None\n",
    "        self.out_weights_1 = None\n",
    "        self.out_biases_1 = None\n",
    "        self.out_weights_2 = None\n",
    "        self.out_weights_2 = None\n",
    "        self.out_biases_2 = None\n",
    "        self.out_weights_3 = None\n",
    "        self.out_biases_3 = None\n",
    "        self.out_weights_4 = None\n",
    "        self.out_biases_4 = None\n",
    "        self.out_weights_5 = None\n",
    "        self.out_biases_5 = None\n",
    "\n",
    "    def init_data(self, picked_file=None):\n",
    "        if picked_file is not None:\n",
    "            self.train_data, self.train_labels, self.test_data, \\\n",
    "                self.test_labels, self.valid_data, self.valid_labels = \\\n",
    "                load_pickled_data(picked_file)\n",
    "\n",
    "    def define_graph(self, keep_pro=0.95, eta=0.05, decay_step=5000, decay_rate=0.95):\n",
    "        \"\"\"\n",
    "        定义图参数\n",
    "        :param keep_pro: DropOut参数\n",
    "        :param eta: 学习率\n",
    "        :param decay_step: 学习率衰减步\n",
    "        :param decay_rate: 学习率衰减率\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.train_graph = tf.Graph()\n",
    "        with self.train_graph.as_default():\n",
    "            # Input Data.\n",
    "            self.tf_train_dataset = tf.placeholder(tf.float32, shape=self.shape)\n",
    "            self.tf_train_labels = tf.placeholder(tf.int32, shape=(self.batch_size, 6))\n",
    "            self.tf_valid_dataset = tf.constant(self.valid_data)\n",
    "            self.tf_test_dataset = tf.constant(self.test_data)\n",
    "            # init varibales\n",
    "            # Conv Layers\n",
    "            self.conv_layer1_weights = tf.get_variable('c_1_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                       self.num_channels, self.depth_1],\n",
    "                                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.conv_layer1_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_1]), name='c_1_b')\n",
    "            self.conv_layer2_weights = tf.get_variable('c_2_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                       self.depth_1, self.depth_2],\n",
    "                                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.conv_layer2_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_2]), name='c_2_b')\n",
    "            self.conv_layer3_weights = tf.get_variable('c_3_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                       self.depth_2, self.num_hidden1],\n",
    "                                                       initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.conv_layer3_biases = tf.Variable(tf.constant(1.0, shape=[self.num_hidden1]), name='c_3_b')\n",
    "            # Output Layer\n",
    "            self.out_weights_len = tf.get_variable('o_len', shape=[self.hidden_num, self.num_labels],\n",
    "                                                   initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_len = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_len'))\n",
    "            self.out_weights_1 = tf.get_variable('o_1', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_1 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_1'))\n",
    "            self.out_weights_2 = tf.get_variable('o_2', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_2 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_2'))\n",
    "            self.out_weights_3 = tf.get_variable('o_3', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_3 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_3'))\n",
    "            self.out_weights_4 = tf.get_variable('o_4', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_4 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_4'))\n",
    "            self.out_weights_5 = tf.get_variable('o_5', shape=[self.hidden_num, self.num_labels],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out_biases_5 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_5'))\n",
    "            # Training computation.\n",
    "            logitslen, logits1, logits2, logits3, logits4, logits5 = \\\n",
    "                self._infer(self.tf_train_dataset, keep_pro, self.shape)\n",
    "            self.loss = \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logitslen, self.tf_train_labels[:, 0])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, self.tf_train_labels[:, 1])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits2, self.tf_train_labels[:, 2])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits3, self.tf_train_labels[:, 3])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits4, self.tf_train_labels[:, 4])) + \\\n",
    "                tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits5, self.tf_train_labels[:, 5]))\n",
    "            # Optimizer.\n",
    "            global_step = tf.Variable(0)\n",
    "            learning_rate = tf.train.exponential_decay(eta, global_step, decay_step, decay_rate)\n",
    "            self.optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(self.loss, global_step=global_step)\n",
    "            # Predictions of the training, validation, and test data.\n",
    "            self.train_prediction = tf.pack(list(map(tf.nn.softmax,\n",
    "                                                     self._infer(self.tf_train_dataset, 1.0, self.shape))))\n",
    "            self.valid_prediction = tf.pack(list(map(tf.nn.softmax,\n",
    "                                                     self._infer(self.tf_valid_dataset, 1.0, self.shape))))\n",
    "            self.test_prediction = tf.pack(list(map(tf.nn.softmax,\n",
    "                                                    self._infer(self.tf_test_dataset, 1.0, self.shape))))\n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "    def train_model(self, save_path=None, save=True, epoch=50000, verbose=False, cross_validate=False):\n",
    "        \"\"\"\n",
    "        训练模型，部署应用的时候不能调用\n",
    "        :param save_path: ckpt数据保存路径\n",
    "        :param save: 是否保存ckpt数据\n",
    "        :param epoch: 训练迭代次数\n",
    "        :param verbose: 显示迭代过程中的中间结果\n",
    "        :param cross_validate: 是否是交叉验证\n",
    "        :return: epoch_index, losses, mini_batch_acc, valid_batch_acc, test_acc\n",
    "        \"\"\"\n",
    "        epoch_index = []\n",
    "        losses = []\n",
    "        mini_batch_acc = []\n",
    "        valid_batch_acc = []\n",
    "        epochs = epoch\n",
    "        start_time = time.time()\n",
    "        with tf.Session(graph=self.train_graph) as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('Initialized all variables')\n",
    "            for e in range(epochs):\n",
    "                offset = (e * self.batch_size) % (self.train_labels.shape[0] - self.batch_size)\n",
    "                batch_data = self.train_data[offset:(offset + self.batch_size), :, :, :]\n",
    "                batch_labels = self.train_labels[offset:(offset + self.batch_size), :]\n",
    "                feed_dict = {self.tf_train_dataset: batch_data, self.tf_train_labels: batch_labels}\n",
    "                _, l, predictions = sess.run([self.optimizer, self.loss, self.train_prediction], feed_dict=feed_dict)\n",
    "                if e % 1000 == 0:\n",
    "                    epoch_index.append(e)\n",
    "                    mini_acc = accuracy_func(predictions, batch_labels)\n",
    "                    mini_batch_acc.append(mini_acc)\n",
    "                    valid_acc = accuracy_func(self.valid_prediction.eval(), self.valid_labels)\n",
    "                    valid_batch_acc.append(valid_acc)\n",
    "                    losses.append(l)\n",
    "                    if verbose:\n",
    "                        print('Minibatch loss at step %d: %f' % (e, l))\n",
    "                        print('Minibatch accuracy: %.1f%%' % mini_acc)\n",
    "                        if not cross_validate:\n",
    "                            print('Validation accuracy: %.1f%%' % valid_acc)\n",
    "            test_acc = accuracy_func(self.test_prediction.eval(), self.test_labels)\n",
    "            print('Test accuracy: %.1f%%' % test_acc)\n",
    "            if save:\n",
    "                self.save_path = self.saver.save(sess, save_path)\n",
    "                print(\"Model saved in file: %s\" % self.save_path)\n",
    "            end_time = time.time()\n",
    "            print('train time: %s' % (end_time - start_time))\n",
    "        return epoch_index, losses, mini_batch_acc, valid_batch_acc, test_acc\n",
    "\n",
    "    def infer_model(self, input_data, ckpt_path):\n",
    "        \"\"\"\n",
    "        infer input data\n",
    "        :param input_data: input a instance\n",
    "        :param ckpt_path: path to the ckpt file\n",
    "        :return: return result\n",
    "        \"\"\"\n",
    "        infer_graph = tf.Graph()\n",
    "        with infer_graph.as_default():\n",
    "            # Input Data.\n",
    "            tf_infer_data = tf.placeholder(tf.float32, shape=(input_data.shape[0], input_data.shape[1],\n",
    "                                                              input_data.shape[2], 1))\n",
    "            # init varibales\n",
    "            conv_layer1_weights = tf.get_variable('c_1_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.num_channels, self.depth_1],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer1_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_1]), name='c_1_b')\n",
    "            conv_layer2_weights = tf.get_variable('c_2_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.depth_1, self.depth_2],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer2_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_2]), name='c_2_b')\n",
    "            conv_layer3_weights = tf.get_variable('c_3_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.depth_2, self.num_hidden1],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer3_biases = tf.Variable(tf.constant(1.0, shape=[self.num_hidden1]), name='c_3_b')\n",
    "            # Output Layer\n",
    "            out_weights_len = tf.get_variable('o_len', shape=[self.hidden_num, self.num_labels],\n",
    "                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_len = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_len'))\n",
    "            out_weights_1 = tf.get_variable('o_1', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_1 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_1'))\n",
    "            out_weights_2 = tf.get_variable('o_2', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_2 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_2'))\n",
    "            out_weights_3 = tf.get_variable('o_3', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_3 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_3'))\n",
    "            out_weights_4 = tf.get_variable('o_4', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_4 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_4'))\n",
    "            out_weights_5 = tf.get_variable('o_5', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_5 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_5'))\n",
    "\n",
    "            def infer(data, keep_prob, d_shape):\n",
    "                # conv layer\n",
    "                lcn = local_contrast_normalization(data, d_shape)\n",
    "                conv_1 = tf.nn.conv2d(lcn, conv_layer1_weights, [1, 1, 1, 1], 'VALID', name='c_1')\n",
    "                conv_1 = tf.nn.relu(conv_1 + conv_layer1_biases)\n",
    "                conv_1 = tf.nn.local_response_normalization(conv_1)\n",
    "                pool_1 = tf.nn.max_pool(conv_1, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_1')\n",
    "                conv_2 = tf.nn.conv2d(pool_1, conv_layer2_weights, [1, 1, 1, 1], padding='VALID', name='c_2')\n",
    "                conv_2 = tf.nn.relu(conv_2 + conv_layer2_biases)\n",
    "                conv_2 = tf.nn.local_response_normalization(conv_2)\n",
    "                pool_2 = tf.nn.max_pool(conv_2, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_2_')\n",
    "                conv_3 = tf.nn.conv2d(pool_2, conv_layer3_weights, [1, 1, 1, 1], padding='VALID', name='c_3')\n",
    "                conv_3 = tf.nn.relu(conv_3 + conv_layer3_biases)\n",
    "                conv_3 = tf.nn.dropout(conv_3, keep_prob)\n",
    "                shapes = conv_3.get_shape().as_list()\n",
    "                hidden = tf.reshape(conv_3, [shapes[0], shapes[1] * shapes[2] * shapes[3]])\n",
    "                # fc layer\n",
    "                logits_len = tf.matmul(hidden, out_weights_len) + out_biases_len\n",
    "                logits_1 = tf.matmul(hidden, out_weights_1) + out_biases_1\n",
    "                logits_2 = tf.matmul(hidden, out_weights_2) + out_biases_2\n",
    "                logits_3 = tf.matmul(hidden, out_weights_3) + out_biases_3\n",
    "                logits_4 = tf.matmul(hidden, out_weights_4) + out_biases_4\n",
    "                logits_5 = tf.matmul(hidden, out_weights_5) + out_biases_5\n",
    "                return logits_len, logits_1, logits_2, logits_3, logits_4, logits_5\n",
    "\n",
    "            logitslen, logits1, logits2, logits3, logits4, logits5 = infer(tf_infer_data, 1.0, self.shape)\n",
    "            # Predictions\n",
    "            softmax_result = list(map(tf.nn.softmax, infer(tf_infer_data, 1.0, self.shape)))\n",
    "            infer_predict = tf.pack(softmax_result)\n",
    "            prediction = tf.transpose(tf.argmax(infer_predict, 2))\n",
    "            self.infer_saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session(graph=infer_graph) as session:\n",
    "            self.infer_saver.restore(session, save_path=ckpt_path)\n",
    "            input_prediction, infer_prediction, l_len, l_1, l_2, l_3, l_4, l_5 = session.run(\n",
    "                [prediction, infer_predict, logitslen, logits1, logits2, logits3, logits4, logits5],\n",
    "                feed_dict={tf_infer_data: input_data})\n",
    "            logits_output = np.array([l_len, l_1, l_2, l_3, l_4, l_5]).reshape((6, 11))\n",
    "            softmax = lambda data: np.exp(data) / np.sum(np.exp(data))\n",
    "            df = pd.DataFrame(np.array([softmax(d) for d in logits_output]).T,\n",
    "                              columns=['length', '1', '2', '3', '4', '5'])\n",
    "            value = list(map(str, range(10)))\n",
    "            value.append('no digit')\n",
    "            df.insert(0, column='softmax', value=value)\n",
    "            return input_prediction, df\n",
    "\n",
    "    def infer_data(self, input_data, input_labels, ckpt_path):\n",
    "        \"\"\"\n",
    "        infer input data\n",
    "        :param input_data: input a instance\n",
    "        :param ckpt_path: path to the ckpt file\n",
    "        :param input_labels: input data labels\n",
    "        :return: return result\n",
    "        \"\"\"\n",
    "        infer_graph = tf.Graph()\n",
    "        with infer_graph.as_default():\n",
    "            # Input Data.\n",
    "            tf_infer_data = tf.placeholder(tf.float32, shape=(input_data.shape[0], input_data.shape[1],\n",
    "                                                              input_data.shape[2], 1))\n",
    "            tf_infer_label = tf.placeholder(tf.int32, shape=(input_labels.shape[0], input_labels.shape[1]))\n",
    "            # init varibales\n",
    "            conv_layer1_weights = tf.get_variable('c_1_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.num_channels, self.depth_1],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer1_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_1]), name='c_1_b')\n",
    "            conv_layer2_weights = tf.get_variable('c_2_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.depth_1, self.depth_2],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer2_biases = tf.Variable(tf.constant(1.0, shape=[self.depth_2]), name='c_2_b')\n",
    "            conv_layer3_weights = tf.get_variable('c_3_w', shape=[self.patch_size, self.patch_size,\n",
    "                                                                  self.depth_2, self.num_hidden1],\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            conv_layer3_biases = tf.Variable(tf.constant(1.0, shape=[self.num_hidden1]), name='c_3_b')\n",
    "            # Output Layer\n",
    "            out_weights_len = tf.get_variable('o_len', shape=[self.hidden_num, self.num_labels],\n",
    "                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_len = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_len'))\n",
    "            out_weights_1 = tf.get_variable('o_1', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_1 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_1'))\n",
    "            out_weights_2 = tf.get_variable('o_2', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_2 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_2'))\n",
    "            out_weights_3 = tf.get_variable('o_3', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_3 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_3'))\n",
    "            out_weights_4 = tf.get_variable('o_4', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_4 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_4'))\n",
    "            out_weights_5 = tf.get_variable('o_5', shape=[self.hidden_num, self.num_labels],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            out_biases_5 = tf.Variable(tf.constant(1.0, shape=[self.num_labels], name='o_b_5'))\n",
    "\n",
    "            def infer(data, keep_prob, d_shape):\n",
    "                # conv layer\n",
    "                lcn = local_contrast_normalization(data, d_shape)\n",
    "                conv_1 = tf.nn.conv2d(lcn, conv_layer1_weights, [1, 1, 1, 1], 'VALID', name='c_1')\n",
    "                conv_1 = tf.nn.relu(conv_1 + conv_layer1_biases)\n",
    "                conv_1 = tf.nn.local_response_normalization(conv_1)\n",
    "                pool_1 = tf.nn.max_pool(conv_1, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_1')\n",
    "                conv_2 = tf.nn.conv2d(pool_1, conv_layer2_weights, [1, 1, 1, 1], padding='VALID', name='c_2')\n",
    "                conv_2 = tf.nn.relu(conv_2 + conv_layer2_biases)\n",
    "                conv_2 = tf.nn.local_response_normalization(conv_2)\n",
    "                pool_2 = tf.nn.max_pool(conv_2, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_2_')\n",
    "                conv_3 = tf.nn.conv2d(pool_2, conv_layer3_weights, [1, 1, 1, 1], padding='VALID', name='c_3')\n",
    "                conv_3 = tf.nn.relu(conv_3 + conv_layer3_biases)\n",
    "                conv_3 = tf.nn.dropout(conv_3, keep_prob)\n",
    "                shapes = conv_3.get_shape().as_list()\n",
    "                hidden = tf.reshape(conv_3, [shapes[0], shapes[1] * shapes[2] * shapes[3]])\n",
    "                # fc layer\n",
    "                logits_len = tf.matmul(hidden, out_weights_len) + out_biases_len\n",
    "                logits_1 = tf.matmul(hidden, out_weights_1) + out_biases_1\n",
    "                logits_2 = tf.matmul(hidden, out_weights_2) + out_biases_2\n",
    "                logits_3 = tf.matmul(hidden, out_weights_3) + out_biases_3\n",
    "                logits_4 = tf.matmul(hidden, out_weights_4) + out_biases_4\n",
    "                logits_5 = tf.matmul(hidden, out_weights_5) + out_biases_5\n",
    "                return logits_len, logits_1, logits_2, logits_3, logits_4, logits_5\n",
    "\n",
    "            logitslen, logits1, logits2, logits3, logits4, logits5 = infer(tf_infer_data, 1.0, self.shape)\n",
    "\n",
    "            # Predictions\n",
    "            _loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logitslen, tf_infer_label[:, 0])) + \\\n",
    "                    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, tf_infer_label[:, 1])) + \\\n",
    "                    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits2, tf_infer_label[:, 2])) + \\\n",
    "                    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits3, tf_infer_label[:, 3])) + \\\n",
    "                    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits4, tf_infer_label[:, 4])) + \\\n",
    "                    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits5, tf_infer_label[:, 5]))\n",
    "            infer_predict = tf.pack(list(map(tf.nn.softmax, infer(tf_infer_data, 1.0, self.shape))))\n",
    "            prediction = tf.transpose(tf.argmax(infer_predict, 2))\n",
    "            self.infer_saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session(graph=infer_graph) as session:\n",
    "            self.infer_saver.restore(session, save_path=ckpt_path)\n",
    "            input_prediction, infer_prediction, loss = session.run([prediction, infer_predict, _loss],\n",
    "                                                                   feed_dict={tf_infer_data: input_data,\n",
    "                                                                              tf_infer_label: input_labels})\n",
    "            accuracy = accuracy_func(infer_prediction, input_labels[:])\n",
    "            return input_prediction, loss, accuracy\n",
    "\n",
    "    def _infer(self, data, keep_prob, d_shape):\n",
    "        \"\"\"\n",
    "        same as infer_data, for training process\n",
    "        :param data: data\n",
    "        :param keep_prob: keep probability for DropOut\n",
    "        :param d_shape: shape of data\n",
    "        :return: logits_1, logits_2, logits_3, logits_4, logits_5\n",
    "        \"\"\"\n",
    "        # conv layer\n",
    "        lcn = local_contrast_normalization(data, d_shape)\n",
    "        conv_1 = tf.nn.conv2d(lcn, self.conv_layer1_weights, [1, 1, 1, 1], 'VALID', name='c_1')\n",
    "        conv_1 = tf.nn.relu(conv_1 + self.conv_layer1_biases)\n",
    "        lrn = tf.nn.local_response_normalization(conv_1)\n",
    "        pool_1 = tf.nn.max_pool(lrn, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_1')\n",
    "        conv_2 = tf.nn.conv2d(pool_1, self.conv_layer2_weights, [1, 1, 1, 1], padding='VALID', name='c_2')\n",
    "        conv_2 = tf.nn.relu(conv_2 + self.conv_layer2_biases)\n",
    "        lrn = tf.nn.local_response_normalization(conv_2)\n",
    "        pool_2 = tf.nn.max_pool(lrn, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='p_2_')\n",
    "        conv_3 = tf.nn.conv2d(pool_2, self.conv_layer3_weights, [1, 1, 1, 1], padding='VALID', name='c_3')\n",
    "        conv_3 = tf.nn.relu(conv_3 + self.conv_layer3_biases)\n",
    "        conv_3 = tf.nn.dropout(conv_3, keep_prob)\n",
    "        shapes = conv_3.get_shape().as_list()\n",
    "        hidden = tf.reshape(conv_3, [shapes[0], shapes[1] * shapes[2] * shapes[3]])\n",
    "        # fc layer\n",
    "        logits_len = tf.matmul(hidden, self.out_weights_len) + self.out_biases_len\n",
    "        logits_1 = tf.matmul(hidden, self.out_weights_1) + self.out_biases_1\n",
    "        logits_2 = tf.matmul(hidden, self.out_weights_2) + self.out_biases_2\n",
    "        logits_3 = tf.matmul(hidden, self.out_weights_3) + self.out_biases_3\n",
    "        logits_4 = tf.matmul(hidden, self.out_weights_4) + self.out_biases_4\n",
    "        logits_5 = tf.matmul(hidden, self.out_weights_5) + self.out_biases_5\n",
    "        return logits_len, logits_1, logits_2, logits_3, logits_4, logits_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((230070, 32, 32, 1), (230070, 6))\n",
      "((13068, 32, 32, 1), (13068, 6))\n",
      "((5684, 32, 32, 1), (5684, 6))\n"
     ]
    }
   ],
   "source": [
    "train_model = MultiDigits('SVHN.pickle')\n",
    "train_model.define_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized all variables\n",
      "Minibatch loss at step 0: 20.546581\n",
      "Minibatch accuracy: 2.9%\n",
      "Validation accuracy: 52.3%\n",
      "Minibatch loss at step 1000: 3.840687\n",
      "Minibatch accuracy: 80.2%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 2000: 2.129208\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 3000: 2.364197\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 4000: 2.519809\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 5000: 1.834348\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 6000: 1.433285\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 7000: 2.926084\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 8000: 1.451802\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 9000: 1.712339\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 10000: 1.228494\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 11000: 1.837456\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 12000: 2.200591\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 13000: 1.862291\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 14000: 1.951806\n",
      "Minibatch accuracy: 90.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 15000: 1.500382\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 16000: 1.607494\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 17000: 1.840317\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 18000: 1.405655\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 19000: 1.450938\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 20000: 1.451707\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 21000: 0.585239\n",
      "Minibatch accuracy: 97.9%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 22000: 1.143198\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 23000: 1.488104\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 24000: 1.347148\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 25000: 1.907906\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 26000: 1.774326\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 27000: 0.893276\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 28000: 1.106320\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 29000: 1.134104\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 30000: 1.275209\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 31000: 1.917496\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 32000: 1.004940\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 33000: 0.945007\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 34000: 1.045919\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 35000: 1.169487\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 36000: 0.754896\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 37000: 0.858228\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 38000: 0.794839\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 39000: 1.972529\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 40000: 1.030765\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 41000: 0.926485\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 42000: 0.947950\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 43000: 1.615797\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 44000: 1.249788\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 45000: 1.521014\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 46000: 0.839290\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 47000: 1.474329\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 48000: 1.129914\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 49000: 1.261985\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 50000: 1.716832\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 51000: 0.965860\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 52000: 1.027365\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 53000: 1.432972\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 54000: 1.318729\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 55000: 1.377559\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 56000: 1.550822\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 57000: 1.377155\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 58000: 1.235118\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 59000: 1.258774\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 60000: 0.970414\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 61000: 1.554058\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 62000: 1.405828\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 63000: 1.497005\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 64000: 0.684048\n",
      "Minibatch accuracy: 97.4%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 65000: 1.068961\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 66000: 1.270772\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 67000: 1.142275\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 68000: 0.972117\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 69000: 1.697083\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 70000: 1.224641\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 71000: 1.079498\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 72000: 0.872715\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 73000: 1.081081\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 74000: 1.006115\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 75000: 1.273713\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 76000: 0.674869\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 77000: 1.016415\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 78000: 1.079306\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 79000: 1.290358\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 80000: 1.132622\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 81000: 0.999178\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 82000: 1.179177\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 83000: 1.220756\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 84000: 1.163870\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 85000: 1.103709\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 86000: 1.289935\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 87000: 0.851628\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 88000: 0.972235\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 89000: 0.701353\n",
      "Minibatch accuracy: 97.1%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 90000: 1.243405\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 91000: 0.613118\n",
      "Minibatch accuracy: 97.9%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 92000: 0.867513\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 93000: 1.497567\n",
      "Minibatch accuracy: 92.7%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 94000: 0.522720\n",
      "Minibatch accuracy: 98.7%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 95000: 1.221720\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 96000: 0.991759\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 97000: 1.140162\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 98000: 1.637047\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 99000: 0.896944\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 100000: 0.618798\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 101000: 0.882256\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 102000: 0.645257\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 103000: 1.304199\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 104000: 1.799861\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 105000: 0.738675\n",
      "Minibatch accuracy: 97.1%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 106000: 1.133646\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 107000: 0.969419\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 108000: 1.035187\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 109000: 0.858678\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 110000: 0.829221\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 111000: 1.332006\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 112000: 0.672492\n",
      "Minibatch accuracy: 97.1%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 113000: 1.336707\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 114000: 1.263458\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 115000: 1.955902\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 116000: 1.023682\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 117000: 0.928816\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 118000: 0.841203\n",
      "Minibatch accuracy: 97.1%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 119000: 1.023074\n",
      "Minibatch accuracy: 97.1%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 120000: 1.083964\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 121000: 0.956263\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 122000: 1.391880\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 123000: 0.779036\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 124000: 0.641281\n",
      "Minibatch accuracy: 98.2%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 125000: 1.450519\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 126000: 0.730452\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 127000: 0.894814\n",
      "Minibatch accuracy: 97.4%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 128000: 1.114565\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 129000: 1.197137\n",
      "Minibatch accuracy: 92.7%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 130000: 1.416818\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 131000: 0.773174\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 132000: 1.083740\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 133000: 0.916619\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 134000: 0.861576\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 135000: 0.873872\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 136000: 0.878584\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 137000: 0.908277\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 138000: 0.808312\n",
      "Minibatch accuracy: 97.1%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 139000: 0.806406\n",
      "Minibatch accuracy: 98.2%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 140000: 1.109602\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 141000: 0.602885\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 142000: 1.093848\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 143000: 0.874318\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 144000: 0.798456\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 145000: 0.597199\n",
      "Minibatch accuracy: 97.9%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 146000: 0.848114\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 147000: 1.439730\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 148000: 0.736174\n",
      "Minibatch accuracy: 97.1%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 149000: 0.848583\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.6%\n",
      "Test accuracy: 93.3%\n",
      "Model saved in file: ckpt_data/SVHN.ckpt\n",
      "train time: 1971.28146195\n"
     ]
    }
   ],
   "source": [
    "epoch_index, losses, mini_batch_acc, valid_batch_acc, test_acc = \\\n",
    "    train_model.train_model(save_path='ckpt_data/SVHN.ckpt', save=True, epoch=150000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1000, 32, 32, 1), (1000, 6))\n"
     ]
    }
   ],
   "source": [
    "# 对前1000个测试数据进行测试\n",
    "indexes = range(1000)\n",
    "input_datas, input_labels = train_model.test_data[indexes, ...], train_model.test_labels[indexes, ...]\n",
    "\n",
    "print(input_datas.shape, input_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model.save_path = \"ckpt_data/SVHN.ckpt\"\n",
    "prediction, loss, accuracy = train_model.infer_data(input_datas, input_labels, ckpt_path=\"ckpt_data/SVHN.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.55718\n"
     ]
    }
   ],
   "source": [
    "print('loss: %s' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 92.083333%\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: %f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_strict(predictions, real_labels):\n",
    "    \"\"\"\n",
    "    如果图片中的数字都识别对了，才算识别出一张图片\n",
    "    \"\"\"\n",
    "    if predictions.shape != real_labels.shape:\n",
    "        raise ValueError(' invalid dimension ')\n",
    "    predict = [to_number(i, 1) for i in [i.tolist() for i in predictions]]\n",
    "    real = [to_number(i, 1) for i in [i.tolist() for i in real_labels]]\n",
    "    return sum(np.array(predict) == np.array(real)) / float(predictions.shape[0])\n",
    "\n",
    "\n",
    "def to_number(result, index=0):\n",
    "    \"\"\"\n",
    "    results to number\n",
    "    :param result: numpy array result, like [[1, 2, 9, 10]]\n",
    "    :param index: 0\n",
    "    :return: number, int type\n",
    "    \"\"\"\n",
    "    return int(''.join(map(str, result[index:result.index(10) if 10 in result else len(result)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strict accuracy: 0.696000%\n"
     ]
    }
   ],
   "source": [
    "print('strict accuracy: %f%%' % accuracy_strict(prediction, input_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
